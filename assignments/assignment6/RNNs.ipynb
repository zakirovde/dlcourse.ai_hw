{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJL2PMl39YfA",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P59NYU98GCb9",
        "colab": {}
      },
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9e5b68cd-90cb-4778-c558-18940780d47e"
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "colab": {},
        "outputId": "d46447e3-664a-47e3-88bf-4812cf1e9c64"
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "66850ae2-784e-44d6-8232-332e38183524"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f2d2bdf-79f7-4ba4-f913-5effbf451b53"
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'CONJ', 'NUM', '.', 'NOUN', 'ADJ', 'ADV', 'PRON', 'X', 'DET', 'VERB', 'ADP', 'PRT'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "7bc0302a-8412-4781-eb39-ebffba0579f2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57855f23-7143-4b02-dee3-d15abe62fdf9"
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "642801de-b5aa-40a6-d8a5-8f67b8797dd0"
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "243586c6-57d1-4695-8db0-efa3e5586d53"
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7LNBIqXELtm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ce65395-dd9c-4f0f-94de-ee74eb0711bc"
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data, backoff=bigram_tagger)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 93.43%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "outputId": "dd3ce868-dfce-4090-e5b1-8fe5fd5fa701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((36, 4), (36, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEHju54d68T",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        # self.hidden_dim = lstm_hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count)\n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "        # self.hidden = self.init_hidden()\n",
        "\n",
        "    # def init_hidden(self):\n",
        "    #     return(torch.zeros(1, 4, self.hidden_dim), torch.zeros(1, 4, self.hidden_dim))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self.word_embeddings(inputs)\n",
        "        # lstm_out, self.hidden = self.lstm(emb, self.hidden)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        return tag_space#F.log_softmax(tag_space, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "colab": {}
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zonU6Oj9ZiNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82cb1a96-c79b-47c1-9ebc-8c9344792c99"
      },
      "source": [
        "y_batch_pad = y_batch != 0\n",
        "preds = torch.argmax(logits, 2)\n",
        "# preds = torch.argmax(F.log_softmax(logits, dim=1), 2)\n",
        "correct = (preds==y_batch)[y_batch_pad].sum()\n",
        "total = y_batch_pad.sum()\n",
        "accuracy = float(correct.item()) / total.item()\n",
        "print(accuracy)"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.06741573033707865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "616e3bf6-6d56-4383-a7d8-cf51dd77a56d"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "loss = criterion(logits.permute(0,2,1), y_batch)\n",
        "print(loss)"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.6146, grad_fn=<NllLoss2DBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = criterion(logits.permute(0,2,1), y_batch)\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                pad = (y_batch != 0).float()\n",
        "                preds = torch.argmax(logits, 2)\n",
        "                # preds = torch.argmax(F.log_softmax(logits, dim=1), 2)\n",
        "                cur_correct_count = ((preds==y_batch).float() * pad).sum()\n",
        "                cur_sum_count = pad.sum()\n",
        "                # accuracy = float(correct.item()) / total.item()\n",
        "\n",
        "                # cur_correct_count, cur_sum_count = <calc accuracy>\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pqfbeh1ltEYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8dea93a1-0ae5-40c6-a198-44c45cb0b448"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.68539, Accuracy = 78.28%: 100%|██████████| 572/572 [00:04<00:00, 116.81it/s]\n",
            "[1 / 50]   Val: Loss = 0.35923, Accuracy = 87.66%: 100%|██████████| 13/13 [00:00<00:00, 88.95it/s]\n",
            "[2 / 50] Train: Loss = 0.27242, Accuracy = 90.99%: 100%|██████████| 572/572 [00:04<00:00, 117.89it/s]\n",
            "[2 / 50]   Val: Loss = 0.24663, Accuracy = 91.41%: 100%|██████████| 13/13 [00:00<00:00, 92.84it/s]\n",
            "[3 / 50] Train: Loss = 0.18376, Accuracy = 93.94%: 100%|██████████| 572/572 [00:04<00:00, 119.04it/s]\n",
            "[3 / 50]   Val: Loss = 0.20985, Accuracy = 92.79%: 100%|██████████| 13/13 [00:00<00:00, 86.43it/s]\n",
            "[4 / 50] Train: Loss = 0.13772, Accuracy = 95.42%: 100%|██████████| 572/572 [00:04<00:00, 117.74it/s]\n",
            "[4 / 50]   Val: Loss = 0.19527, Accuracy = 93.48%: 100%|██████████| 13/13 [00:00<00:00, 93.74it/s]\n",
            "[5 / 50] Train: Loss = 0.10750, Accuracy = 96.41%: 100%|██████████| 572/572 [00:04<00:00, 116.16it/s]\n",
            "[5 / 50]   Val: Loss = 0.18618, Accuracy = 93.95%: 100%|██████████| 13/13 [00:00<00:00, 90.50it/s]\n",
            "[6 / 50] Train: Loss = 0.08598, Accuracy = 97.13%: 100%|██████████| 572/572 [00:04<00:00, 117.46it/s]\n",
            "[6 / 50]   Val: Loss = 0.18228, Accuracy = 94.17%: 100%|██████████| 13/13 [00:00<00:00, 91.56it/s]\n",
            "[7 / 50] Train: Loss = 0.06947, Accuracy = 97.67%: 100%|██████████| 572/572 [00:04<00:00, 119.41it/s]\n",
            "[7 / 50]   Val: Loss = 0.19108, Accuracy = 94.29%: 100%|██████████| 13/13 [00:00<00:00, 88.72it/s]\n",
            "[8 / 50] Train: Loss = 0.05659, Accuracy = 98.09%: 100%|██████████| 572/572 [00:04<00:00, 117.83it/s]\n",
            "[8 / 50]   Val: Loss = 0.19315, Accuracy = 94.40%: 100%|██████████| 13/13 [00:00<00:00, 89.22it/s]\n",
            "[9 / 50] Train: Loss = 0.04638, Accuracy = 98.43%: 100%|██████████| 572/572 [00:04<00:00, 117.35it/s]\n",
            "[9 / 50]   Val: Loss = 0.20107, Accuracy = 94.41%: 100%|██████████| 13/13 [00:00<00:00, 89.60it/s]\n",
            "[10 / 50] Train: Loss = 0.03834, Accuracy = 98.71%: 100%|██████████| 572/572 [00:04<00:00, 116.80it/s]\n",
            "[10 / 50]   Val: Loss = 0.21141, Accuracy = 94.36%: 100%|██████████| 13/13 [00:00<00:00, 90.02it/s]\n",
            "[11 / 50] Train: Loss = 0.03126, Accuracy = 98.96%: 100%|██████████| 572/572 [00:04<00:00, 117.31it/s]\n",
            "[11 / 50]   Val: Loss = 0.22338, Accuracy = 94.39%: 100%|██████████| 13/13 [00:00<00:00, 86.85it/s]\n",
            "[12 / 50] Train: Loss = 0.02565, Accuracy = 99.16%: 100%|██████████| 572/572 [00:04<00:00, 117.23it/s]\n",
            "[12 / 50]   Val: Loss = 0.23249, Accuracy = 94.36%: 100%|██████████| 13/13 [00:00<00:00, 79.73it/s]\n",
            "[13 / 50] Train: Loss = 0.02093, Accuracy = 99.33%: 100%|██████████| 572/572 [00:04<00:00, 116.91it/s]\n",
            "[13 / 50]   Val: Loss = 0.24455, Accuracy = 94.31%: 100%|██████████| 13/13 [00:00<00:00, 85.75it/s]\n",
            "[14 / 50] Train: Loss = 0.01719, Accuracy = 99.47%: 100%|██████████| 572/572 [00:04<00:00, 116.59it/s]\n",
            "[14 / 50]   Val: Loss = 0.24854, Accuracy = 94.35%: 100%|██████████| 13/13 [00:00<00:00, 89.74it/s]\n",
            "[15 / 50] Train: Loss = 0.01396, Accuracy = 99.58%: 100%|██████████| 572/572 [00:04<00:00, 115.51it/s]\n",
            "[15 / 50]   Val: Loss = 0.25299, Accuracy = 94.33%: 100%|██████████| 13/13 [00:00<00:00, 91.19it/s]\n",
            "[16 / 50] Train: Loss = 0.01139, Accuracy = 99.65%: 100%|██████████| 572/572 [00:04<00:00, 116.78it/s]\n",
            "[16 / 50]   Val: Loss = 0.27201, Accuracy = 94.30%: 100%|██████████| 13/13 [00:00<00:00, 92.05it/s]\n",
            "[17 / 50] Train: Loss = 0.00975, Accuracy = 99.71%: 100%|██████████| 572/572 [00:04<00:00, 116.21it/s]\n",
            "[17 / 50]   Val: Loss = 0.28411, Accuracy = 94.30%: 100%|██████████| 13/13 [00:00<00:00, 82.76it/s]\n",
            "[18 / 50] Train: Loss = 0.00844, Accuracy = 99.74%: 100%|██████████| 572/572 [00:04<00:00, 116.85it/s]\n",
            "[18 / 50]   Val: Loss = 0.29335, Accuracy = 94.27%: 100%|██████████| 13/13 [00:00<00:00, 90.35it/s]\n",
            "[19 / 50] Train: Loss = 0.00727, Accuracy = 99.78%: 100%|██████████| 572/572 [00:04<00:00, 117.00it/s]\n",
            "[19 / 50]   Val: Loss = 0.30114, Accuracy = 94.24%: 100%|██████████| 13/13 [00:00<00:00, 89.45it/s]\n",
            "[20 / 50] Train: Loss = 0.00671, Accuracy = 99.79%: 100%|██████████| 572/572 [00:04<00:00, 116.53it/s]\n",
            "[20 / 50]   Val: Loss = 0.32206, Accuracy = 94.15%: 100%|██████████| 13/13 [00:00<00:00, 91.88it/s]\n",
            "[21 / 50] Train: Loss = 0.00600, Accuracy = 99.82%: 100%|██████████| 572/572 [00:04<00:00, 116.94it/s]\n",
            "[21 / 50]   Val: Loss = 0.32248, Accuracy = 94.19%: 100%|██████████| 13/13 [00:00<00:00, 92.08it/s]\n",
            "[22 / 50] Train: Loss = 0.00597, Accuracy = 99.81%: 100%|██████████| 572/572 [00:04<00:00, 116.08it/s]\n",
            "[22 / 50]   Val: Loss = 0.33008, Accuracy = 94.19%: 100%|██████████| 13/13 [00:00<00:00, 93.46it/s]\n",
            "[23 / 50] Train: Loss = 0.00582, Accuracy = 99.81%: 100%|██████████| 572/572 [00:04<00:00, 115.34it/s]\n",
            "[23 / 50]   Val: Loss = 0.34309, Accuracy = 94.14%: 100%|██████████| 13/13 [00:00<00:00, 87.95it/s]\n",
            "[24 / 50] Train: Loss = 0.00567, Accuracy = 99.81%: 100%|██████████| 572/572 [00:04<00:00, 117.69it/s]\n",
            "[24 / 50]   Val: Loss = 0.35216, Accuracy = 94.26%: 100%|██████████| 13/13 [00:00<00:00, 87.59it/s]\n",
            "[25 / 50] Train: Loss = 0.00529, Accuracy = 99.82%: 100%|██████████| 572/572 [00:04<00:00, 115.96it/s]\n",
            "[25 / 50]   Val: Loss = 0.35677, Accuracy = 94.16%: 100%|██████████| 13/13 [00:00<00:00, 86.62it/s]\n",
            "[26 / 50] Train: Loss = 0.00481, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 117.82it/s]\n",
            "[26 / 50]   Val: Loss = 0.35257, Accuracy = 94.24%: 100%|██████████| 13/13 [00:00<00:00, 93.99it/s]\n",
            "[27 / 50] Train: Loss = 0.00489, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 115.83it/s]\n",
            "[27 / 50]   Val: Loss = 0.35185, Accuracy = 94.28%: 100%|██████████| 13/13 [00:00<00:00, 78.12it/s]\n",
            "[28 / 50] Train: Loss = 0.00471, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 115.99it/s]\n",
            "[28 / 50]   Val: Loss = 0.36642, Accuracy = 94.16%: 100%|██████████| 13/13 [00:00<00:00, 88.76it/s]\n",
            "[29 / 50] Train: Loss = 0.00499, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 116.15it/s]\n",
            "[29 / 50]   Val: Loss = 0.37940, Accuracy = 94.23%: 100%|██████████| 13/13 [00:00<00:00, 86.19it/s]\n",
            "[30 / 50] Train: Loss = 0.00487, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 117.46it/s]\n",
            "[30 / 50]   Val: Loss = 0.38812, Accuracy = 94.25%: 100%|██████████| 13/13 [00:00<00:00, 84.87it/s]\n",
            "[31 / 50] Train: Loss = 0.00485, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 117.56it/s]\n",
            "[31 / 50]   Val: Loss = 0.38392, Accuracy = 94.19%: 100%|██████████| 13/13 [00:00<00:00, 91.91it/s]\n",
            "[32 / 50] Train: Loss = 0.00459, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 115.53it/s]\n",
            "[32 / 50]   Val: Loss = 0.38383, Accuracy = 94.21%: 100%|██████████| 13/13 [00:00<00:00, 90.44it/s]\n",
            "[33 / 50] Train: Loss = 0.00426, Accuracy = 99.84%: 100%|██████████| 572/572 [00:04<00:00, 117.04it/s]\n",
            "[33 / 50]   Val: Loss = 0.39994, Accuracy = 94.25%: 100%|██████████| 13/13 [00:00<00:00, 90.48it/s]\n",
            "[34 / 50] Train: Loss = 0.00417, Accuracy = 99.84%: 100%|██████████| 572/572 [00:04<00:00, 116.16it/s]\n",
            "[34 / 50]   Val: Loss = 0.39331, Accuracy = 94.26%: 100%|██████████| 13/13 [00:00<00:00, 89.57it/s]\n",
            "[35 / 50] Train: Loss = 0.00454, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 112.71it/s]\n",
            "[35 / 50]   Val: Loss = 0.40194, Accuracy = 94.22%: 100%|██████████| 13/13 [00:00<00:00, 86.13it/s]\n",
            "[36 / 50] Train: Loss = 0.00479, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 114.94it/s]\n",
            "[36 / 50]   Val: Loss = 0.38742, Accuracy = 94.29%: 100%|██████████| 13/13 [00:00<00:00, 91.36it/s]\n",
            "[37 / 50] Train: Loss = 0.00453, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 116.50it/s]\n",
            "[37 / 50]   Val: Loss = 0.39322, Accuracy = 94.28%: 100%|██████████| 13/13 [00:00<00:00, 90.64it/s]\n",
            "[38 / 50] Train: Loss = 0.00436, Accuracy = 99.84%: 100%|██████████| 572/572 [00:04<00:00, 117.34it/s]\n",
            "[38 / 50]   Val: Loss = 0.38561, Accuracy = 94.33%: 100%|██████████| 13/13 [00:00<00:00, 86.54it/s]\n",
            "[39 / 50] Train: Loss = 0.00431, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 116.27it/s]\n",
            "[39 / 50]   Val: Loss = 0.39045, Accuracy = 94.22%: 100%|██████████| 13/13 [00:00<00:00, 91.93it/s]\n",
            "[40 / 50] Train: Loss = 0.00444, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 117.12it/s]\n",
            "[40 / 50]   Val: Loss = 0.40588, Accuracy = 94.21%: 100%|██████████| 13/13 [00:00<00:00, 90.16it/s]\n",
            "[41 / 50] Train: Loss = 0.00402, Accuracy = 99.84%: 100%|██████████| 572/572 [00:04<00:00, 115.74it/s]\n",
            "[41 / 50]   Val: Loss = 0.39389, Accuracy = 94.33%: 100%|██████████| 13/13 [00:00<00:00, 90.65it/s]\n",
            "[42 / 50] Train: Loss = 0.00409, Accuracy = 99.84%: 100%|██████████| 572/572 [00:04<00:00, 116.90it/s]\n",
            "[42 / 50]   Val: Loss = 0.39671, Accuracy = 94.29%: 100%|██████████| 13/13 [00:00<00:00, 93.25it/s]\n",
            "[43 / 50] Train: Loss = 0.00386, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 116.05it/s]\n",
            "[43 / 50]   Val: Loss = 0.40727, Accuracy = 94.28%: 100%|██████████| 13/13 [00:00<00:00, 91.77it/s]\n",
            "[44 / 50] Train: Loss = 0.00390, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 115.57it/s]\n",
            "[44 / 50]   Val: Loss = 0.39065, Accuracy = 94.37%: 100%|██████████| 13/13 [00:00<00:00, 88.44it/s]\n",
            "[45 / 50] Train: Loss = 0.00389, Accuracy = 99.84%: 100%|██████████| 572/572 [00:04<00:00, 116.19it/s]\n",
            "[45 / 50]   Val: Loss = 0.40187, Accuracy = 94.31%: 100%|██████████| 13/13 [00:00<00:00, 87.67it/s]\n",
            "[46 / 50] Train: Loss = 0.00519, Accuracy = 99.80%: 100%|██████████| 572/572 [00:04<00:00, 116.78it/s]\n",
            "[46 / 50]   Val: Loss = 0.38698, Accuracy = 94.30%: 100%|██████████| 13/13 [00:00<00:00, 85.80it/s]\n",
            "[47 / 50] Train: Loss = 0.00464, Accuracy = 99.82%: 100%|██████████| 572/572 [00:04<00:00, 116.18it/s]\n",
            "[47 / 50]   Val: Loss = 0.39546, Accuracy = 94.32%: 100%|██████████| 13/13 [00:00<00:00, 87.09it/s]\n",
            "[48 / 50] Train: Loss = 0.00387, Accuracy = 99.84%: 100%|██████████| 572/572 [00:04<00:00, 118.00it/s]\n",
            "[48 / 50]   Val: Loss = 0.38511, Accuracy = 94.46%: 100%|██████████| 13/13 [00:00<00:00, 91.86it/s]\n",
            "[49 / 50] Train: Loss = 0.00371, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 116.42it/s]\n",
            "[49 / 50]   Val: Loss = 0.39036, Accuracy = 94.38%: 100%|██████████| 13/13 [00:00<00:00, 93.98it/s]\n",
            "[50 / 50] Train: Loss = 0.00364, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 117.17it/s]\n",
            "[50 / 50]   Val: Loss = 0.38410, Accuracy = 94.45%: 100%|██████████| 13/13 [00:00<00:00, 92.62it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kemUkMd1vx7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48478a6c-2021-431f-b778-5855aa15983a"
      },
      "source": [
        "def evaluate_test(model, X, y):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (X_batch, y_batch) in enumerate(iterate_batches((X, y), 4)):\n",
        "        X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "        logits = model(X_batch)\n",
        "        pad = (y_batch != 0).float()\n",
        "        preds = torch.argmax(logits, 2)\n",
        "        correct += ((preds==y_batch).float() * pad).sum()\n",
        "        total += pad.sum()\n",
        "    return float(correct) / total\n",
        "\n",
        "evaluate_test(model, X_test, y_test)"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9456, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsIejB729Yf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        # self.hidden_dim = lstm_hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
        "        # self.hidden = self.init_hidden()\n",
        "\n",
        "    # def init_hidden(self):\n",
        "    #     return(torch.zeros(1, 4, self.hidden_dim), torch.zeros(1, 4, self.hidden_dim))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self.word_embeddings(inputs)\n",
        "        # lstm_out, self.hidden = self.lstm(emb, self.hidden)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        return tag_space#F.log_softmax(tag_space, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6EANnvpeu7Zp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e126507-ad8b-45f4-ade0-1af9f59e33de"
      },
      "source": [
        "model = BidirectionalLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.55173, Accuracy = 82.50%: 100%|██████████| 572/572 [00:06<00:00, 94.29it/s]\n",
            "[1 / 50]   Val: Loss = 0.27185, Accuracy = 91.32%: 100%|██████████| 13/13 [00:00<00:00, 71.25it/s]\n",
            "[2 / 50] Train: Loss = 0.20367, Accuracy = 93.53%: 100%|██████████| 572/572 [00:06<00:00, 93.98it/s]\n",
            "[2 / 50]   Val: Loss = 0.17688, Accuracy = 94.37%: 100%|██████████| 13/13 [00:00<00:00, 73.06it/s]\n",
            "[3 / 50] Train: Loss = 0.12865, Accuracy = 96.02%: 100%|██████████| 572/572 [00:06<00:00, 93.58it/s]\n",
            "[3 / 50]   Val: Loss = 0.13809, Accuracy = 95.61%: 100%|██████████| 13/13 [00:00<00:00, 71.55it/s]\n",
            "[4 / 50] Train: Loss = 0.08791, Accuracy = 97.32%: 100%|██████████| 572/572 [00:06<00:00, 94.76it/s]\n",
            "[4 / 50]   Val: Loss = 0.12067, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 76.47it/s]\n",
            "[5 / 50] Train: Loss = 0.06071, Accuracy = 98.19%: 100%|██████████| 572/572 [00:06<00:00, 93.54it/s]\n",
            "[5 / 50]   Val: Loss = 0.11286, Accuracy = 96.43%: 100%|██████████| 13/13 [00:00<00:00, 70.93it/s]\n",
            "[6 / 50] Train: Loss = 0.04149, Accuracy = 98.82%: 100%|██████████| 572/572 [00:06<00:00, 94.52it/s]\n",
            "[6 / 50]   Val: Loss = 0.11405, Accuracy = 96.57%: 100%|██████████| 13/13 [00:00<00:00, 72.81it/s]\n",
            "[7 / 50] Train: Loss = 0.02795, Accuracy = 99.24%: 100%|██████████| 572/572 [00:06<00:00, 94.22it/s]\n",
            "[7 / 50]   Val: Loss = 0.11296, Accuracy = 96.58%: 100%|██████████| 13/13 [00:00<00:00, 74.05it/s]\n",
            "[8 / 50] Train: Loss = 0.01831, Accuracy = 99.53%: 100%|██████████| 572/572 [00:06<00:00, 94.51it/s]\n",
            "[8 / 50]   Val: Loss = 0.11709, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 71.56it/s]\n",
            "[9 / 50] Train: Loss = 0.01175, Accuracy = 99.72%: 100%|██████████| 572/572 [00:06<00:00, 94.86it/s]\n",
            "[9 / 50]   Val: Loss = 0.12257, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 77.36it/s]\n",
            "[10 / 50] Train: Loss = 0.00743, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 94.81it/s]\n",
            "[10 / 50]   Val: Loss = 0.13298, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 76.07it/s]\n",
            "[11 / 50] Train: Loss = 0.00477, Accuracy = 99.92%: 100%|██████████| 572/572 [00:06<00:00, 94.19it/s]\n",
            "[11 / 50]   Val: Loss = 0.13891, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 75.37it/s]\n",
            "[12 / 50] Train: Loss = 0.00302, Accuracy = 99.96%: 100%|██████████| 572/572 [00:06<00:00, 93.90it/s]\n",
            "[12 / 50]   Val: Loss = 0.14391, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 73.69it/s]\n",
            "[13 / 50] Train: Loss = 0.00182, Accuracy = 99.98%: 100%|██████████| 572/572 [00:06<00:00, 95.07it/s]\n",
            "[13 / 50]   Val: Loss = 0.15339, Accuracy = 96.63%: 100%|██████████| 13/13 [00:00<00:00, 74.56it/s]\n",
            "[14 / 50] Train: Loss = 0.00135, Accuracy = 99.99%: 100%|██████████| 572/572 [00:05<00:00, 95.40it/s]\n",
            "[14 / 50]   Val: Loss = 0.15804, Accuracy = 96.59%: 100%|██████████| 13/13 [00:00<00:00, 74.17it/s]\n",
            "[15 / 50] Train: Loss = 0.00199, Accuracy = 99.96%: 100%|██████████| 572/572 [00:06<00:00, 94.25it/s]\n",
            "[15 / 50]   Val: Loss = 0.16747, Accuracy = 96.38%: 100%|██████████| 13/13 [00:00<00:00, 76.17it/s]\n",
            "[16 / 50] Train: Loss = 0.00320, Accuracy = 99.92%: 100%|██████████| 572/572 [00:06<00:00, 93.47it/s]\n",
            "[16 / 50]   Val: Loss = 0.17297, Accuracy = 96.54%: 100%|██████████| 13/13 [00:00<00:00, 72.74it/s]\n",
            "[17 / 50] Train: Loss = 0.00126, Accuracy = 99.98%: 100%|██████████| 572/572 [00:06<00:00, 92.39it/s]\n",
            "[17 / 50]   Val: Loss = 0.17756, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 69.02it/s]\n",
            "[18 / 50] Train: Loss = 0.00051, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 93.63it/s]\n",
            "[18 / 50]   Val: Loss = 0.17128, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 74.45it/s]\n",
            "[19 / 50] Train: Loss = 0.00026, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 94.14it/s]\n",
            "[19 / 50]   Val: Loss = 0.17565, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 73.59it/s]\n",
            "[20 / 50] Train: Loss = 0.00017, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 94.34it/s]\n",
            "[20 / 50]   Val: Loss = 0.17828, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 72.05it/s]\n",
            "[21 / 50] Train: Loss = 0.00284, Accuracy = 99.91%: 100%|██████████| 572/572 [00:06<00:00, 94.80it/s]\n",
            "[21 / 50]   Val: Loss = 0.18637, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 68.56it/s]\n",
            "[22 / 50] Train: Loss = 0.00277, Accuracy = 99.92%: 100%|██████████| 572/572 [00:06<00:00, 94.10it/s]\n",
            "[22 / 50]   Val: Loss = 0.18402, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 67.42it/s]\n",
            "[23 / 50] Train: Loss = 0.00043, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 94.75it/s]\n",
            "[23 / 50]   Val: Loss = 0.18285, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 72.06it/s]\n",
            "[24 / 50] Train: Loss = 0.00017, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 94.61it/s]\n",
            "[24 / 50]   Val: Loss = 0.18632, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 73.72it/s]\n",
            "[25 / 50] Train: Loss = 0.00012, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 93.81it/s]\n",
            "[25 / 50]   Val: Loss = 0.18732, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 72.76it/s]\n",
            "[26 / 50] Train: Loss = 0.00009, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 94.74it/s]\n",
            "[26 / 50]   Val: Loss = 0.18861, Accuracy = 96.85%: 100%|██████████| 13/13 [00:00<00:00, 74.18it/s]\n",
            "[27 / 50] Train: Loss = 0.00008, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 94.00it/s]\n",
            "[27 / 50]   Val: Loss = 0.19419, Accuracy = 96.85%: 100%|██████████| 13/13 [00:00<00:00, 73.68it/s]\n",
            "[28 / 50] Train: Loss = 0.00008, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 95.07it/s]\n",
            "[28 / 50]   Val: Loss = 0.19527, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 75.78it/s]\n",
            "[29 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 96.14it/s]\n",
            "[29 / 50]   Val: Loss = 0.19717, Accuracy = 96.85%: 100%|██████████| 13/13 [00:00<00:00, 74.91it/s]\n",
            "[30 / 50] Train: Loss = 0.00526, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 94.67it/s]\n",
            "[30 / 50]   Val: Loss = 0.19149, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 74.02it/s]\n",
            "[31 / 50] Train: Loss = 0.00197, Accuracy = 99.94%: 100%|██████████| 572/572 [00:06<00:00, 94.79it/s]\n",
            "[31 / 50]   Val: Loss = 0.18728, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 73.37it/s]\n",
            "[32 / 50] Train: Loss = 0.00034, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 95.81it/s]\n",
            "[32 / 50]   Val: Loss = 0.18841, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 72.53it/s]\n",
            "[33 / 50] Train: Loss = 0.00009, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 95.87it/s]\n",
            "[33 / 50]   Val: Loss = 0.19046, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 78.31it/s]\n",
            "[34 / 50] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 96.07it/s]\n",
            "[34 / 50]   Val: Loss = 0.19182, Accuracy = 96.89%: 100%|██████████| 13/13 [00:00<00:00, 73.58it/s]\n",
            "[35 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 95.12it/s]\n",
            "[35 / 50]   Val: Loss = 0.19288, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 72.67it/s]\n",
            "[36 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 95.36it/s]\n",
            "[36 / 50]   Val: Loss = 0.19554, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 73.91it/s]\n",
            "[37 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 93.03it/s]\n",
            "[37 / 50]   Val: Loss = 0.19738, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 72.34it/s]\n",
            "[38 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 93.68it/s]\n",
            "[38 / 50]   Val: Loss = 0.19833, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 68.94it/s]\n",
            "[39 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 95.25it/s]\n",
            "[39 / 50]   Val: Loss = 0.19927, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 74.27it/s]\n",
            "[40 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 94.98it/s]\n",
            "[40 / 50]   Val: Loss = 0.20435, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 74.37it/s]\n",
            "[41 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 94.68it/s]\n",
            "[41 / 50]   Val: Loss = 0.21522, Accuracy = 96.88%: 100%|██████████| 13/13 [00:00<00:00, 71.03it/s]\n",
            "[42 / 50] Train: Loss = 0.00592, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 95.39it/s]\n",
            "[42 / 50]   Val: Loss = 0.21799, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 75.01it/s]\n",
            "[43 / 50] Train: Loss = 0.00091, Accuracy = 99.97%: 100%|██████████| 572/572 [00:06<00:00, 95.24it/s]\n",
            "[43 / 50]   Val: Loss = 0.21242, Accuracy = 96.88%: 100%|██████████| 13/13 [00:00<00:00, 75.42it/s]\n",
            "[44 / 50] Train: Loss = 0.00020, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 94.34it/s]\n",
            "[44 / 50]   Val: Loss = 0.21338, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 71.55it/s]\n",
            "[45 / 50] Train: Loss = 0.00013, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 93.14it/s]\n",
            "[45 / 50]   Val: Loss = 0.20929, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 70.84it/s]\n",
            "[46 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 94.02it/s]\n",
            "[46 / 50]   Val: Loss = 0.21381, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 75.22it/s]\n",
            "[47 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 93.47it/s]\n",
            "[47 / 50]   Val: Loss = 0.21422, Accuracy = 96.98%: 100%|██████████| 13/13 [00:00<00:00, 72.40it/s]\n",
            "[48 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 94.02it/s]\n",
            "[48 / 50]   Val: Loss = 0.21511, Accuracy = 96.99%: 100%|██████████| 13/13 [00:00<00:00, 71.99it/s]\n",
            "[49 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 93.96it/s]\n",
            "[49 / 50]   Val: Loss = 0.21603, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 71.43it/s]\n",
            "[50 / 50] Train: Loss = 0.00021, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 93.12it/s]\n",
            "[50 / 50]   Val: Loss = 0.22107, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 71.00it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "76295ded-e324-48ba-85ae-64ec5b767d94",
        "id": "u3yjkqkTu7Zt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "evaluate_test(model, X_test, y_test)"
      ],
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9666, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "00e73a14-4796-411d-d452-a5f32481416b"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2928c2b5-be73-425a-c161-a61a42b62efd"
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxaRBpQd0pat",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        # self.hidden_dim = lstm_hidden_dim\n",
        "        self.word_embeddings = nn.Embedding.from_pretrained(FloatTensor(embeddings))\n",
        "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
        "        # self.hidden = self.init_hidden()\n",
        "\n",
        "    # def init_hidden(self):\n",
        "    #     return(torch.zeros(1, 4, self.hidden_dim), torch.zeros(1, 4, self.hidden_dim))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self.word_embeddings(inputs)\n",
        "        # lstm_out, self.hidden = self.lstm(emb, self.hidden)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        return tag_space#F.log_softmax(tag_space, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "1afbdbf6-4682-434e-c253-364898f11619"
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 20] Train: Loss = 0.56949, Accuracy = 83.50%: 100%|██████████| 572/572 [00:05<00:00, 102.99it/s]\n",
            "[1 / 20]   Val: Loss = 0.26125, Accuracy = 92.30%: 100%|██████████| 13/13 [00:00<00:00, 89.40it/s]\n",
            "[2 / 20] Train: Loss = 0.19153, Accuracy = 94.35%: 100%|██████████| 572/572 [00:05<00:00, 97.80it/s] \n",
            "[2 / 20]   Val: Loss = 0.17650, Accuracy = 94.69%: 100%|██████████| 13/13 [00:00<00:00, 86.88it/s]\n",
            "[3 / 20] Train: Loss = 0.13551, Accuracy = 95.95%: 100%|██████████| 572/572 [00:05<00:00, 102.51it/s]\n",
            "[3 / 20]   Val: Loss = 0.14173, Accuracy = 95.71%: 100%|██████████| 13/13 [00:00<00:00, 90.07it/s]\n",
            "[4 / 20] Train: Loss = 0.10871, Accuracy = 96.72%: 100%|██████████| 572/572 [00:05<00:00, 103.28it/s]\n",
            "[4 / 20]   Val: Loss = 0.12494, Accuracy = 96.11%: 100%|██████████| 13/13 [00:00<00:00, 93.68it/s]\n",
            "[5 / 20] Train: Loss = 0.09278, Accuracy = 97.17%: 100%|██████████| 572/572 [00:05<00:00, 102.47it/s]\n",
            "[5 / 20]   Val: Loss = 0.11446, Accuracy = 96.43%: 100%|██████████| 13/13 [00:00<00:00, 90.88it/s]\n",
            "[6 / 20] Train: Loss = 0.08279, Accuracy = 97.47%: 100%|██████████| 572/572 [00:05<00:00, 99.25it/s] \n",
            "[6 / 20]   Val: Loss = 0.10670, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 92.15it/s]\n",
            "[7 / 20] Train: Loss = 0.07508, Accuracy = 97.69%: 100%|██████████| 572/572 [00:05<00:00, 98.72it/s]\n",
            "[7 / 20]   Val: Loss = 0.10282, Accuracy = 96.75%: 100%|██████████| 13/13 [00:00<00:00, 90.53it/s]\n",
            "[8 / 20] Train: Loss = 0.06912, Accuracy = 97.87%: 100%|██████████| 572/572 [00:05<00:00, 102.15it/s]\n",
            "[8 / 20]   Val: Loss = 0.09968, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 86.40it/s]\n",
            "[9 / 20] Train: Loss = 0.06428, Accuracy = 98.01%: 100%|██████████| 572/572 [00:05<00:00, 101.67it/s]\n",
            "[9 / 20]   Val: Loss = 0.09701, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 90.41it/s]\n",
            "[10 / 20] Train: Loss = 0.06020, Accuracy = 98.13%: 100%|██████████| 572/572 [00:05<00:00, 101.84it/s]\n",
            "[10 / 20]   Val: Loss = 0.09411, Accuracy = 97.02%: 100%|██████████| 13/13 [00:00<00:00, 93.39it/s]\n",
            "[11 / 20] Train: Loss = 0.05655, Accuracy = 98.23%: 100%|██████████| 572/572 [00:05<00:00, 102.87it/s]\n",
            "[11 / 20]   Val: Loss = 0.09444, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 90.40it/s]\n",
            "[12 / 20] Train: Loss = 0.05344, Accuracy = 98.33%: 100%|██████████| 572/572 [00:05<00:00, 103.47it/s]\n",
            "[12 / 20]   Val: Loss = 0.09303, Accuracy = 97.06%: 100%|██████████| 13/13 [00:00<00:00, 88.88it/s]\n",
            "[13 / 20] Train: Loss = 0.05076, Accuracy = 98.40%: 100%|██████████| 572/572 [00:05<00:00, 101.78it/s]\n",
            "[13 / 20]   Val: Loss = 0.09638, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 91.06it/s]\n",
            "[14 / 20] Train: Loss = 0.04797, Accuracy = 98.48%: 100%|██████████| 572/572 [00:05<00:00, 102.26it/s]\n",
            "[14 / 20]   Val: Loss = 0.09145, Accuracy = 97.08%: 100%|██████████| 13/13 [00:00<00:00, 88.68it/s]\n",
            "[15 / 20] Train: Loss = 0.04573, Accuracy = 98.56%: 100%|██████████| 572/572 [00:05<00:00, 100.34it/s]\n",
            "[15 / 20]   Val: Loss = 0.09202, Accuracy = 97.12%: 100%|██████████| 13/13 [00:00<00:00, 92.28it/s]\n",
            "[16 / 20] Train: Loss = 0.04340, Accuracy = 98.64%: 100%|██████████| 572/572 [00:05<00:00, 99.59it/s] \n",
            "[16 / 20]   Val: Loss = 0.09332, Accuracy = 97.11%: 100%|██████████| 13/13 [00:00<00:00, 87.47it/s]\n",
            "[17 / 20] Train: Loss = 0.04153, Accuracy = 98.70%: 100%|██████████| 572/572 [00:05<00:00, 101.40it/s]\n",
            "[17 / 20]   Val: Loss = 0.09494, Accuracy = 97.11%: 100%|██████████| 13/13 [00:00<00:00, 90.48it/s]\n",
            "[18 / 20] Train: Loss = 0.03973, Accuracy = 98.76%: 100%|██████████| 572/572 [00:05<00:00, 100.62it/s]\n",
            "[18 / 20]   Val: Loss = 0.09423, Accuracy = 97.09%: 100%|██████████| 13/13 [00:00<00:00, 89.93it/s]\n",
            "[19 / 20] Train: Loss = 0.03789, Accuracy = 98.81%: 100%|██████████| 572/572 [00:05<00:00, 101.66it/s]\n",
            "[19 / 20]   Val: Loss = 0.09574, Accuracy = 97.09%: 100%|██████████| 13/13 [00:00<00:00, 95.60it/s]\n",
            "[20 / 20] Train: Loss = 0.03622, Accuracy = 98.86%: 100%|██████████| 572/572 [00:05<00:00, 102.80it/s]\n",
            "[20 / 20]   Val: Loss = 0.09594, Accuracy = 97.03%: 100%|██████████| 13/13 [00:00<00:00, 85.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "626a2229-47bf-4fbc-e89a-43cd4eae678c"
      },
      "source": [
        "evaluate_test(model, X_test, y_test)"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9704, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 321
        }
      ]
    }
  ]
}